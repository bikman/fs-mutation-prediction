[general]
# select proteins train/eval set
# 1:ALL_PROTEINS_LIST
# 2:NORMAL_PROTEINS_LIST
# 3:COMPETITIVE_GROWTH_ASSAY_LIST
# 4:ANTIBIOTICS_RESISTANCE_LIST
# 5:DEBUG_LIST
protein_set = 1
#index of the protein file (1 to 39)
#to use for eval / fine tuning
eval_protein_file_number = 39
# number of protein in train set
# set to 39 or more to disable
train_set_count = 39
#choose seed for random splits
seed = 1234
#0=no scheduler, 1=with scheduler
use_scheduler = 0
#gamma parameter for LR scheduler
gamma = 0.5
#step parameter for LR scheduler
step = 300
#the folder with all the pickled data
dump_root = /root/code/dumps
#default value 768
seq_emb_size = 768
#model:
#1: PrismScoreEmbDiffSimpleModel
#2: PrismScoreDeltasOnlyModel
#3: PrismScoreDeltasEmbDiffModel
#4: PrismScoreDeltasEmbModel
#5: PrismScoreNoDDGModel
#6: PrismScoreNoDDEModel
#7: PrismScoreNoDeltasModel
model = 1
#number of layers in transformer encoders
attn_len = 3
# length of embedding diff tensor (+-)
diff_len = 15
# 0=use regular diffs, 1=use pdb indexed diffs
use_pdb = 0
# use encoder on all deltas block
use_deltas_encoder = 0
# number of channels in model
cz = 64
# number of bins
bins = 10

[flow_data_creation]
# percentage of variants to be taken from data
variants_cutoff = 100
#index of protein to create data
#use -1 for 'all'
protein_id = -1
# 1 = normalize scores, 0 = do not normalize
normalize_scores = 0
# 1 = normalize deltas, 0 = do not normalize
normalize_deltas = 0


[fine_tuning_data_creation]
#1=per mutation, 2=all mutations per position
eval_data_type = 1
#0=take eval data randomly,
#1=take only eval data from destructive mutations
destructive_data_only = 1
#how many items to take from list
# 1, 2, 4, 16, 32
data_count = 256
# 2 = normalize ds, 1 = normalize scores, 0 = do not normalize
normalize_scores = 0
# 1 = normalize deltas, 0 = do not normalize
normalize_deltas = 0
# add max orig score mutation to FT sample
add_max_v = 1


[flow_train]
batch_size = 48
heads = 8
epochs = 300
patience = 10000
#learning rate
lr = 0.0001
#use_fine_tune_data:
#0=use only train data,
#1=use also fine tune addition
use_fine_tune_data = 0
#loss parameter
alpha=0.75
#1=apply batch norm during train
batch_norm = 0

[flow_pretrained]
#the folder with pickled previously trained model
pretrain_folder = /root/code/model_pretrain
#is_enabled: 0=training new model, 1=continue pretrained
is_enabled = 0
#learning rate
lr = 0.00005

[flow_fine_tune]
#the folder with pickled previously trained model
fine_tune_folder = /root/code/model_fine_tune
#learning rate
lr = 0.0001
epochs = 30
loops = 10
#loss parameter
alpha=0.75
#1=apply batch norm during fine tuning
batch_norm = 0
# use min/max replacement in FT sample
use_min_max = 1


[flow_eval]
batch_size = 48
heads = 8
#use_fine_tune_data:
#0=full eval data
#1=use fine tuned eval data
use_fine_tune_data = 0


[episodes_train]
episodes = 1000
num_ways = 4
support_set_size = 32
query_set_size = 256
num_inner_epochs = 5
use_pretrain = 0
#learning rate used for inner loops
inner_lr = 0.0001
#1=use validation protein in episode
use_validation = 1
use_scheduler = 0
#1=use normalized scores per episode
norm_episode = 0


